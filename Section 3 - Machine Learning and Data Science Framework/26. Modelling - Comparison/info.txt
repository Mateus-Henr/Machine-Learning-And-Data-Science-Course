26. Modelling - Comparison
This happens in the last split of machine learning, the test data split. For testing, we usually give the algorithm, data sets that it has never seen before and analyse it. It's not uncommon to see a slightly difference in performance between the training set and the test set, however if this difference is too wide, then there's a problem. They are called:
	• Underfitting - Occurs when the machine learning algorithm does not fit so well to the training data, that it does not generalize well to the testing data. Like it is taking some wild guesses.
	• Overfitting - Occurs when the machine learning algorithm fits so closely to the training data, that it does not generalize well to the testing data. Like it memorized.

Reasons for the missmatch above:
	• Data leakage - This is when the raining data leaks to the test data (overfitting usually).
	• Data mismatch - Normally happens when you have different features in the training data to the test data (underfitting).
	• Another examples that might cause underfitting:
		○ Try a more advanced model
		○ Increase model hyperparameters
		○ Reduce amount of features
		○ Train longer
	• Reducing overfitting:
		○ Collect more data
		○ Try a less advanced model

When comparing model, always take into account the accuracy, training time and prediction time.
A model should head towards generality.